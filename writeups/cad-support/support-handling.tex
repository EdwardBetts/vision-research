\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{color}
\usepackage{graphics, epstopdf}
\usepackage[pdftex]{graphicx}
\usepackage[pdftex]{hyperref}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\urlstyle{same}

\newcommand{\com}[1]{
	\vspace{0.2cm}
	\noindent
	\emph{#1}
}
\newcommand{\Bern}[1]{\text{Bern}(#1)}

\definecolor{gray}{gray}{0.5}
\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black, urlcolor=gray}

\newcommand{\code}[1]{\texttt{#1}}

\newtheorem{mydef}{Definition}
    
\begin{document}

%\title{Support handling}

%\author{Gustav Larsson}

\section{Support handling}
One of the advantages of training a Bernoulli mixture model of an object from CAD images (in RGB$\alpha$), is that in each image we know the support of the image. This document is a treatment of how to incorporate this knowledge of the support into the model.
%
%Using wavelet deformations of mixture component templates, to 
%Classifying well-posed handwritten digits (MNIST) using binary edge features and wavelet-based deformations. 
%
%\keywords{score following, real-time score alignment, automatic accompaniment}
%
%\chapter{Support handling}

%\section{Introduction}

\subsection{Object model}
Suppose we have a Bernoulli mixture model with parameters $\theta^* \in [0, 1]^{K \times M \times E}$, where $K$ is the number of mixture components, $M$ is the number of pixels, and $E$ is the number of binary features per pixel. Suppose also that each component is associated with a binary support, which is governed by yet another Bernoulli mixture model with parameters $\alpha \in [0, 1]^{K \times M}$.

When trying to learn these parameters, we can imagine drawing a sample of the support, $A$, and then a feature, $X$:
\begin{align*}
    A_{k,j} & \sim \Bern{\alpha_{k,j}}, \\
    X_{k,j,e} & \sim \left\{
        \begin{array}{l l}
            \Bern{\theta^*_{k,j,e}} & \text{if }A_{k,j} = 1  \\
            0 & \text{if }A_{k,j} = 0
        \end{array} \right.
\end{align*}
Together they simplify to 
\[
    X_{k,j,e} \sim \Bern{\theta^*_{k,j,e} \alpha_{k,j}}.
\] 
This is what we are drawing from when training from CAD images, so let us assume that we are able to infer $\theta_{k,j,e} = \theta^*_{k,j,e} \alpha_{k,j}$. Since we know the support of each image, we can also infer $\alpha$.

\subsection{Background model adjustment}
Suppose we have a background model, with Bernoulli probabilities $b_e$ for each feature $e$. Ideally, we would like to have a model of our object drawn from the following distributions
\begin{align*}
    A_{k,j} & \sim \Bern{\alpha_{k,j}}, \\
    Y_{k,j,e} & \sim \left\{
        \begin{array}{l l}
            \Bern{\theta^*_{k,j,e}} & \text{if }A_{k,j} = 1  \\
            \Bern{b_e} & \text{if }A_{k,j} = 0 
        \end{array} \right.
\end{align*}
Let's simplify this to $Y_{k,j,e} \sim \Bern{\theta'_{k,j,e}}$. We get
\begin{align*}
    \theta'_{k,j,e} &= \Pr(Y_{k,j,e} = 1) \\
        &= \Pr(Y_{k,j,e}  = 1| A_{k,j} = 1) \Pr(A_{k,j} = 1) + \Pr(Y_{k,j,e} = 1| A_{k,j} = 0) \Pr(A_{k,j} = 0) \\
        &= \theta^*_{k,j,e} \alpha_{k,j} + b_e (1 - \alpha_{k,j}) = \theta_{k,j,e} + b_e (1 - \alpha_{k,j}).
\end{align*}
Since we know $\theta$, $\alpha$ and $b$, we now know $\theta'$.




\section{Feature spreading}
\newcommand{\neigh}[1]{\text{Ne}(#1)}
\newcommand{\Ne}{\text{Ne}}
\newcommand{\spr}[1]{#1^{\text{(spr)}}}

Feature spreading is done by OR:ing a neighborhood of features $X$. Let $\neigh{j}$ denote a set of all spatial indices in some neighborhood around $j$. Let us regard the result after spreading as a set of new random variables, $\spr{X}$, by the following
\[
    \spr{X}_{k,j,e} = \bigvee_{l \in \neigh{j}} X_{k,l,e}
\]
As before, we want to express this as $\spr{X}_{k,j,e} \sim \Bern{\phi_{k,j,e}}$. We assume all $X$ are independent, so we can simplify as follows 
\begin{align*}
    \phi_{k,j,e} &= \Pr(\spr{X}_{k,j,e} = 1) = \Pr(\bigcup_{l \in \neigh{j}} X_{k,l,e} = 1) = 1 - \Pr(\bigcap_{l \in \neigh{j}} X_{k,l,e} = 0) \\
    &= 1 - \prod_{l \in \neigh{j}} \Pr(X_{k,l,e} = 0) = 1 - \prod_{l \in \neigh{j}} (1 - \theta^*_{k,l,e} \alpha_{k,l}),
\end{align*}
or simply
\[
    \phi_{k,j,e} = 1 - \prod_{l \in \neigh{j}} (1 - \theta_{k,l,e}).
\]

\subsection{Background model adjustment}
%Extending this to a background-aware model $Y'$, as follows
Again, let us extend this to a model, $\spr{Y}$, including background
\[
    \spr{Y}_{k,j,e} = \bigvee_{l \in \neigh{j}} Y_{k,j,e}
\]
Following the same procedure as before, or solving it simply with variable substitution, we get that if $\spr{Y}_{k,j,e} \sim \Bern{\phi'_{k,j,e}}$, then
\begin{equation} \label{eq:phiprime}
    \phi'_{k,j,e} = 1 - \prod_{l \in \neigh{j}} [1 - \theta_{k,l,e} - b_e (1 - \alpha_{k,l})]
\end{equation}
This is unfortunately impossible to express in term of only $\phi$ instead of $\theta$, so we can't adjust this model correctly if we have trained our model with spreading. However, we can still train without spreading to uncover $\theta$, and then build our model using the above expression.

\subsection{Spread background model}
Since we will be spreading our feature maps, it might make sense to infer the background model from the spread features directly. We therefore imagine a spread background model, $\spr{H_0} \sim \Bern{\spr{b}_e}$, with the following relationship to $b_e$ (let $j$ be an arbitrary pixel)
\[
    \spr{b}_e = 1 - \prod_{l \in \neigh{j}} (1 - b_e) = 1 - (1 - b_e)^{|\Ne|},
\]
we can thus recover $b_e$ for use in (\ref{eq:phiprime}) as
\[
    b_e = 1 - (1 - \spr{b}_e)^{\frac{1}{|\Ne|}}.
\]
In the case of a rectangular neighborhood of radius $S$, we have $|\Ne| = (2S + 1)^2$.

%It is faster to infer our background model from 

% There are two potential problems with this, (1) We need to train with more images to 
\subsection{Implementation details}
There are two concerns with this that arise when we depart from our theoretical model. First of all, will we recover $\theta$ correctly without the stabilizing effect of spreading? This is not hard to overcome, either by increasing the number of CAD images as needed, or actually using spreading when training the mixture model, but then taking that result and applying it to unspread edges to retrieve~$\theta$. 

The second concern is the computational overhead. This also turns out to be a non-issue, at least if we assume a rectangular neighborhood. First, we rewrite (\ref{eq:phiprime}) as 
\[
    \log(1 - \phi'_{k,j,e}) = \sum_{l \in \neigh{j}} \log [1 - \theta_{k,l,e} - b_e (1 - \alpha_{k,l})] \equiv \sum_{l \in \neigh{j}} A_{k,l,e},
\]
then, by building an integral image of $A$, it becomes fast to evaluate the above for all spatial locations.
%we can then evaluate the above quickly for all spatial locations by building an integral image of $A$.  %using an integral image of the summand. 




%\subsection{Max pooling - fixed support}


%We make the assumption that in order to observe a feature for component $k$, position $i$ and feature $e$, we have to have $\alpha_{k,i,e} = 1$.

%We make the assumption that $\theta^*_{k,i,e} = 0$ (for all $e$) if $\alpha_{k,i} = 0$.

\end{document}
