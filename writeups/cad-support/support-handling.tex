\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{color}
\usepackage{graphics, epstopdf}
\usepackage[pdftex]{graphicx}
\usepackage[pdftex]{hyperref}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\urlstyle{same}

\newcommand{\com}[1]{
	\vspace{0.2cm}
	\noindent
	\emph{#1}
}
\newcommand{\Bern}[1]{\text{Bern}(#1)}

\definecolor{gray}{gray}{0.5}
\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black, urlcolor=gray}

\newcommand{\code}[1]{\texttt{#1}}

\newtheorem{mydef}{Definition}
    
\begin{document}

\title{Support handling}

\author{Gustav Larsson}

\maketitle

\begin{abstract}
One of the advantages of training a Bernoulli mixture model of an object from CAD images (in RGB$\alpha$), is that in each image we know the support of the image. This document is a treatment of how to incorporate this knowledge of the support into the model.
%
%Using wavelet deformations of mixture component templates, to 
%Classifying well-posed handwritten digits (MNIST) using binary edge features and wavelet-based deformations. 
%
%\keywords{score following, real-time score alignment, automatic accompaniment}
\end{abstract}
%
%\chapter{Support handling}

%\section{Introduction}

\section{Model}
Suppose we have a Bernoulli mixture model with parameters $\theta^* \in [0, 1]^{K \times M \times E}$, where $K$ is the number of mixture components, $M$ is the number of pixels, and $E$ is the number of binary features per pixel. Suppose also that each component is associated with a binary support, which is associated with yet another Bernoulli mixture model with parameters $\alpha \in [0, 1]^{K \times M}$.

When trying to learn these parameters, we can imagine drawing a sample of the support, $A$, and then a feature, $X$:
\begin{align*}
    A_{k,j} & \sim \Bern{\alpha_{k,j}}, \\
    X_{k,j,e} & \sim \left\{
        \begin{array}{l l}
            \Bern{\theta^*_{k,j,e}} & \text{if }A_{k,j} = 1  \\
            0 & \text{if }A_{k,j} = 0
        \end{array} \right.
\end{align*}
Together they simplify to 
\[
    X_{k,j,e} \sim \Bern{\theta^*_{k,j,e} \alpha_{k,j}}.
\] 
This is what we are drawing from when training from CAD images, so let us assume that we are able to infer $\theta_{k,j,e} = \theta^*_{k,j,e} \alpha_{k,j}$. Since we know the support of each image, we can also infer $\alpha$.

\subsection{Background model}
Suppose we have a background model, with Bernoulli probabilities $b_e$ for each feature $e$. Ideally, we would like to have a model of our object drawn from the following distributions
\begin{align*}
    A_{k,j} & \sim \Bern{\alpha_{k,j}}, \\
    Y_{k,j,e} & \sim \left\{
        \begin{array}{l l}
            \Bern{\theta^*_{k,j,e}} & \text{if }A_{k,j} = 1  \\
            \Bern{b_e} & \text{if }A_{k,j} = 0 
        \end{array} \right.
\end{align*}
Let's simplify this to $Y_{k,j,e} \sim \Bern{\theta'_{k,j,e}}$. We get
\begin{align*}
    \theta'_{k,j,e} &= \Pr(Y_{k,j,e} = 1) \\
        &= \Pr(Y_{k,j,e}  = 1| A_{k,j} = 1) \Pr(A_{k,j} = 1) + \Pr(Y_{k,j,e} = 1| A_{k,j} = 0) \Pr(A_{k,j} = 0) \\
        &= \theta^*_{k,j,e} \alpha_{k,j} + b_e (1 - \alpha_{k,j}) = \theta_{k,j,e} + b_e (1 - \alpha_{k,j}).
\end{align*}
Since we know $\theta$, $\alpha$ and $b$, we now know $\theta'$.

%\subsection{Max pooling - fixed support}


%We make the assumption that in order to observe a feature for component $k$, position $i$ and feature $e$, we have to have $\alpha_{k,i,e} = 1$.

%We make the assumption that $\theta^*_{k,i,e} = 0$ (for all $e$) if $\alpha_{k,i} = 0$.

\end{document}
